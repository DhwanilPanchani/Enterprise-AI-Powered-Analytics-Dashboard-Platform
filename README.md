# Data Intelligence Hub

## 1. Project Purpose

The Data Intelligence Hub is a full-stack, AI-powered enterprise analytics platform designed to solve the critical problem of **data silos** in mid-to-large businesses. It serves as a unified central hub where data from multiple sources can be ingested, processed, and analyzed, providing actionable insights through an interactive dashboard and natural language queries.

This platform empowers business analysts, operations managers, and data teams by providing a single source of truth, breaking down departmental barriers to information, and leveraging AI to make data-driven decision-making faster and more intuitive.

## 2. Key Features

-   **Microservice Architecture:** The platform is built using independent, containerized services for Users and Data Ingestion, ensuring scalability and maintainability.
-   **Secure User Authentication:** A complete registration and login system using JSON Web Tokens (JWT) secures access to the dashboard and data endpoints.
-   **Generic Data Ingestion:** A flexible API endpoint can accept a wide variety of JSON-formatted data (e.g., sales, user events, IoT data) and store it efficiently.
-   **Interactive Data Visualization:** A real-time dashboard fetches and displays ingested data in a dynamic bar chart, providing an immediate visual understanding of key metrics.
-   **AI-Powered Natural Language Query (NLQ):** Users can ask questions about their data in plain English (e.g., "how many sales did we have?") and receive concise answers generated by Google's Gemini LLM.
-   **Fully Containerized:** The entire platform is containerized with Docker and orchestrated with Docker Compose, ensuring a consistent development environment and simplifying deployment.

## 3. Architecture & Technical Implications

This project demonstrates a modern, scalable web architecture with several key concepts:

-   **Microservices:** We separated the **User Service** from the **Data Ingestion Service**. This has powerful implications:
    -   **The Right Tool for the Job:** We used PostgreSQL for the User Service (a relational database perfect for structured user data) and MongoDB for the Ingestion Service (a NoSQL database ideal for varied, unstructured data).
    -   **Scalability & Resilience:** Each service can be scaled, updated, or restarted independently without affecting the other. If the Ingestion Service is under heavy load, it won't slow down user logins.
-   **Containerization (Docker):** By containerizing each service, we guarantee that the application runs the same way on a developer's laptop as it does in the cloud. This eliminates "it works on my machine" problems and makes deployment predictable and reliable.
-   **Secure Inter-Service Communication:** The frontend authenticates against the User Service to get a JWT. It then uses that same token to securely request data from the Ingestion Service, which independently verifies the token's signature using a shared secret key. This is a standard pattern for securing microservice ecosystems.

## 4. Tech Stack

#### **Frontend**
-   **Framework:** React (loaded via CDN)
-   **Styling:** Tailwind CSS
-   **Data Visualization:** Chart.js

#### **Backend - User Service**
-   **Framework:** Python / Django / Django REST Framework
-   **Database:** PostgreSQL
-   **Authentication:** Simple JWT (JSON Web Tokens)
-   **Production Server:** Gunicorn

#### **Backend - Ingestion Service**
-   **Framework:** Python / Flask
-   **Database:** MongoDB
-   **AI / LLM:** Google Gemini API
-   **Production Server:** Gunicorn

#### **DevOps & Deployment**
-   **Containerization:** Docker & Docker Compose
-   **Hosting:** Render.com (Services), Neon (PostgreSQL), MongoDB Atlas (MongoDB), Netlify (Frontend)
-   **Version Control:** Git & GitHub

## 5. Running the Project Locally

1.  Clone the repository.
2.  Ensure you have Docker and Docker Compose installed.
3.  Create a `.env` file in the `analytics-platform` root or add your `GEMINI_API_KEY` and `DJANGO_SECRET_KEY` to the `docker-compose.yml` file.
4.  From the root directory, run the command:
    ```bash
    docker compose up --build
    ```
5.  The User Service will be available at `http://localhost:8000` and the Ingestion Service at `http://localhost:5001`.

## 6. API Endpoints

-   `POST /api/register/`: Creates a new user.
-   `POST /api/login/`: Authenticates a user and returns JWT tokens.
-   `POST /api/ingest`: Ingests a JSON object into the raw data collection.
-   `GET /api/data`: (Protected) Retrieves all ingested data.
-   `POST /api/nlq`: (Protected) Accepts a natural language question and returns an AI-generated answer.
